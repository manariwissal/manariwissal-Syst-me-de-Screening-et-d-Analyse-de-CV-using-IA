{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Story Concept: Adil & Harry Learn Resume Screening\n",
    "Adil is an experienced Python developer and machine learning enthusiast. Harry, his curious friend, wants to understand how companies automatically screen resumes. Adil decides to teach him using a project they’ll build together step by step..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scene 1: Starting the Journey\n",
    "# Adil: \"Let’s begin, Harry! First, we’ll import the tools we need.\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Harry: \"So numpy is for math, pandas is for handling tables, and those two are for graphs, right?\"\n",
    "# Adil: \"Exactly! You’re already getting it!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scene 2: Loading the Resume Data\n",
    "# Adil: \"Now Harry, let’s load our resume dataset into a DataFrame so we can explore it.\n",
    "df = pd.read_csv('UpdatedResumeDataSet.csv')\n",
    "\n",
    "# Adil: \"Let’s peek at the first few rows to get a sense of the data.\"\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harry: \"Cool! So each row is a resume?\"\n",
    "# Adil: \"Exactly. Each resume has a 'Resume' column with text and a 'Category' that tells the field — like Data Science, HR, etc.\"\n",
    "\n",
    "# Let’s also check how many rows and columns we have.\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of resumes:\", df.shape[0])\n",
    "print(\"Number of columns:\", df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scene 3: Exploring Resume Categories\n",
    "# Adil: \"Alright Harry, now let's see how many resumes belong to each job field or category.\"\n",
    "\n",
    "df['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the categories\n",
    "category_counts = df['Category'].value_counts()\n",
    "\n",
    "# Display counts\n",
    "print(category_counts)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=category_counts.index, y=category_counts.values, palette=\"viridis\")\n",
    "plt.title(\"Number of Resumes per Category\", fontsize=16)\n",
    "plt.xlabel(\"Job Category\", fontsize=12)\n",
    "plt.ylabel(\"Number of Resumes\", fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Adil Explains:\n",
    "\n",
    "> \"Harry, this bar chart gives us a quick overview of which fields are most common in our dataset. It's useful because if we’re building a classifier later, we should know if the dataset is balanced or skewed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scene 4: Visualizing Resume Categories\n",
    "# Adil: \"Harry, let’s turn the resume categories into a visual story.\"\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.countplot(x='Category', data=df, palette='Set2')\n",
    "plt.title(\"Resume Categories Count\", fontsize=16)\n",
    "plt.xlabel(\"Category\", fontsize=12)\n",
    "plt.ylabel(\"Number of Resumes\", fontsize=12)\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Story Moment\n",
    "> Harry: “Wow! That’s much easier to understand than just looking at numbers.”\n",
    "\n",
    "> Adil: “Exactly! Data visualizations help us communicate insights quickly — especially useful if we want to present this to a recruiter or team.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Scene 5: Discovering Unique Categories\n",
    "# Adil: \"Harry, before we build anything, we should know what types of resume categories we're working with.\"\n",
    "unique_categories = df['Category'].unique()\n",
    "print(\"Unique Resume Categories:\")\n",
    "print(unique_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "> Harry: “So 'unique()' just lists all the different job fields we have?”\n",
    "\n",
    "> Adil: “Exactly. This helps us understand what classes our future model might need to predict.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Unique Categories:\", len(unique_categories))\n",
    "print(\"Categories:\")\n",
    "for idx, cat in enumerate(unique_categories, 1):\n",
    "    print(f\"{idx}. {cat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scene 6: Pie Chart Trouble — and Fixing It\n",
    "> Harry: “Adil, I tried making a pie chart, but something feels off. The colors don’t match all categories!”\n",
    "\n",
    "> Adil: “Good catch, Harry! The issue is in the color array — you're only generating 3 colors, but we probably have more categories.”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "counts = df['Category'].value_counts()\n",
    "labels = counts.index\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.pie(counts, labels=labels, autopct='%1.1f%%', shadow=True,\n",
    "        colors=plt.cm.plasma(np.linspace(0, 1, len(labels))))  # dynamically adjust colors\n",
    "\n",
    "plt.title(\"Distribution of Resume Categories\", fontsize=16)\n",
    "plt.axis('equal')  # Makes the pie chart a perfect circle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Story Moment\n",
    "> Harry: “Ah, now it shows all the categories properly — and with fancy colors too!”\n",
    "\n",
    "> Adil: “Exactly! Always make sure your color array matches the number of items you're plotting. It’s small things like these that make visualizations more accurate and professional.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scene 7: Peeking Into a Resume\n",
    "# Adil: \"Let’s inspect the very first resume in our dataset, Harry.\"\n",
    "\n",
    "print(\"Category:\", df['Category'][0])\n",
    "print(\"\\nResume Preview:\\n\")\n",
    "print(df['Resume'][0][:1000])  # Limit output to first 1000 characters to avoid too much clutter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adil Explains:\n",
    "> \"Each resume here is a long chunk of text, usually unstructured. It contains skills, education, experience — all mixed up! Our job is to process and clean this text before we can use it for training a machine learning model.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview a cleaner snippet\n",
    "resume_text = df['Resume'][0]\n",
    "print(resume_text[:500])  # first 500 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 5 sample resumes and their categories\n",
    "for i in range(5):\n",
    "    print(f\"\\n--- Resume {i+1} ---\")\n",
    "    print(\"Category:\", df['Category'][i])\n",
    "    print(\"Text Preview:\", df['Resume'][i][:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Story Moment\n",
    "> Harry: \"Whoa, this resume is just one giant blob of text!\"\n",
    "\n",
    "> Adil: \"Yup! And that’s why we need Natural Language Processing (NLP) — to clean it, break it down, and make it useful for machines to understand.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scene 8: Balancing Resume Categories\n",
    "# Adil: \"Harry, remember how some categories had way fewer resumes than others?\"\n",
    "# Adil: \"If we train a model on that, it might get biased towards the bigger categories.\"\n",
    "# Adil: \"To fix this, we'll balance the dataset by oversampling smaller categories — copying some resumes until all categories have the same number.\"\n",
    "\n",
    "print(\"Original Category Distribution:\")\n",
    "print(df['Category'].value_counts())\n",
    "\n",
    "# Find the max number of resumes in any category\n",
    "max_size = df['Category'].value_counts().max()\n",
    "\n",
    "# Oversample smaller categories to match max_size\n",
    "balanced_df = df.groupby('Category').apply(lambda x: x.sample(max_size, replace=True)).reset_index(drop=True)\n",
    "\n",
    "# Shuffle to mix the rows randomly after oversampling\n",
    "df = balanced_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nBalanced Category Distribution (After Oversampling):\")\n",
    "print(df['Category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why Oversampling?\n",
    "> Harry: \"Why do we copy some resumes, though? Isn’t that cheating?\"\n",
    "\n",
    "> Adil: \"Good question! It's a trade-off — we risk overfitting by repeating data, but it prevents the model from ignoring smaller classes. Another option is to collect more data or try advanced techniques like SMOTE, but oversampling is a great start.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data:\n",
    "1 URLs, <br>\n",
    "2 hashtags, <br>\n",
    "3 mentions, <br>\n",
    "4 special letters, <br>\n",
    "5 punctuations: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scene 9: Cleaning the Resumes with Regex Magic\n",
    "# Adil: \"Harry, resumes can be messy! They may have URLs, hashtags, mentions, special characters, and random punctuation.\"\n",
    "# Adil: \"We need to clean all of that so our machine learning model doesn't get confused.\"\n",
    "\n",
    "import re\n",
    "\n",
    "def cleanResume(txt):\n",
    "    # Remove URLs like http://something\n",
    "    cleanText = re.sub(r'http\\S+\\s?', ' ', txt)\n",
    "    \n",
    "    # Remove Twitter RT (retweet) or cc if any\n",
    "    cleanText = re.sub(r'RT|cc', ' ', cleanText)\n",
    "    \n",
    "    # Remove hashtags (#something)\n",
    "    cleanText = re.sub(r'#\\S+\\s?', ' ', cleanText)\n",
    "    \n",
    "    # Remove mentions (@username)\n",
    "    cleanText = re.sub(r'@\\S+', ' ', cleanText)\n",
    "    \n",
    "    # Remove punctuation and special characters\n",
    "    cleanText = re.sub(r'[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', cleanText)\n",
    "    \n",
    "    # Remove non-ASCII characters (like emojis)\n",
    "    cleanText = re.sub(r'[^\\x00-\\x7f]', ' ', cleanText)\n",
    "    \n",
    "    # Replace multiple spaces with single space\n",
    "    cleanText = re.sub(r'\\s+', ' ', cleanText)\n",
    "    \n",
    "    # Strip leading/trailing spaces\n",
    "    cleanText = cleanText.strip()\n",
    "    \n",
    "    return cleanText\n",
    "\n",
    "# Test the function with an example\n",
    "test_text = \"my #### $ #  #noorsaeed webiste like is this http://heloword and access it @gmain.com\"\n",
    "print(cleanResume(test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation from Adil:\n",
    "> “See Harry, this cleaning step removes all the unnecessary noise in the resumes — like links, usernames, hashtags, and weird punctuation. That way, when we teach the computer, it focuses on the important words.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean all resumes in the dataset\n",
    "df['Resume'] = df['Resume'].apply(cleanResume)\n",
    "\n",
    "# Check cleaned version of first resume\n",
    "print(df['Resume'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# words into categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scene 10: Turning Categories into Numbers\n",
    "# Adil: \"Harry, computers understand numbers better than text.\"\n",
    "# Adil: \"So we convert our job categories like 'Data Science' or 'HR' into numeric labels.\"\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Learn mapping from text labels to numbers\n",
    "le.fit(df['Category'])\n",
    "\n",
    "# Transform categories into numeric labels\n",
    "df['Category'] = le.transform(df['Category'])\n",
    "\n",
    "# Check unique numeric labels\n",
    "print(\"Unique numeric categories:\", df['Category'].unique())\n",
    "\n",
    "# Optionally, check mapping for reference\n",
    "print(\"Category mapping:\")\n",
    "for i, label in enumerate(le.classes_):\n",
    "    print(f\"{label} --> {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adil Explains:\n",
    "> \"LabelEncoder converts categories into numbers so our model can work with them. The numbers themselves don’t have mathematical meaning — they’re just IDs.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vactorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scene 11: Converting Text to Numbers with TF-IDF\n",
    "# Adil: \"Harry, words themselves can't be directly used by ML models.\"\n",
    "# Adil: \"So we turn resumes into numbers that capture how important words are using TF-IDF.\"\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize vectorizer to ignore common English stopwords\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Learn vocabulary and idf from resumes\n",
    "tfidf.fit(df['Resume'])\n",
    "\n",
    "# Transform all resumes into TF-IDF feature matrix (sparse matrix)\n",
    "X = tfidf.transform(df['Resume'])\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adil Explains:\n",
    "> \"TF-IDF stands for Term Frequency-Inverse Document Frequency — it gives higher scores to words that are important in a resume but rare across all resumes. This way, the model focuses on unique skills or keywords.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scene 12: Splitting Data into Training and Testing Sets\n",
    "# Adil: \"Harry, to check if our model really learns, we split data into training and testing sets.\"\n",
    "# Adil: \"The model trains on the training set and we test it on unseen data — the test set.\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 'X' is our feature matrix, 'y' are the labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,                   # Features (TF-IDF matrix)\n",
    "    df['Category'],      # Target labels\n",
    "    test_size=0.2,       # 20% data for testing\n",
    "    random_state=42      # For reproducibility\n",
    ")\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adil Explains:\n",
    "\"We keep 80% of data for training and 20% for testing. The `random_state` ensures the split is the same every time you run this.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let’s train the model and print the classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scene 13: Training the KNN Model & Evaluating Performance\n",
    "# Adil: \"Harry, now we’ll train our first model: K-Nearest Neighbors.\"\n",
    "# Adil: \"It looks at the closest resumes to decide the category of a new one.\"\n",
    "# Adil: \"Since we have multiple categories, we use OneVsRestClassifier to handle them.\"\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Convert sparse matrices to dense arrays if needed\n",
    "X_train_dense = X_train.toarray() if hasattr(X_train, 'toarray') else X_train\n",
    "X_test_dense = X_test.toarray() if hasattr(X_test, 'toarray') else X_test\n",
    "\n",
    "# Initialize and train the model\n",
    "knn_model = OneVsRestClassifier(KNeighborsClassifier())\n",
    "knn_model.fit(X_train_dense, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_knn = knn_model.predict(X_test_dense)\n",
    "\n",
    "# Evaluate results\n",
    "print(\"\\nKNeighborsClassifier Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_knn):.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_knn))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adil Explains:\n",
    "> “KNN works by looking at the ‘neighbors’ of a resume — the most similar ones — to decide the category. The classification report shows precision, recall, and F1-score for each category so we can see how well it’s performing.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scene 14: Training the Support Vector Classifier (SVC)\n",
    "# Adil: \"Harry, SVM is a powerful model that tries to find the best boundary to separate categories.\"\n",
    "# Adil: \"We’ll train it the same way as before, using OneVsRestClassifier to handle multiple classes.\"\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Train the SVC model\n",
    "svc_model = OneVsRestClassifier(SVC())\n",
    "svc_model.fit(X_train_dense, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_svc = svc_model.predict(X_test_dense)\n",
    "\n",
    "# Evaluate results\n",
    "print(\"\\nSVC Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_svc):.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_svc))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adil Explains:\n",
    "> “SVM tries to maximize the margin between different categories, making it robust and effective for text classification.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scene 15: Training the Random Forest Classifier\n",
    "# Adil: \"Harry, Random Forest builds many decision trees and combines their votes to improve accuracy.\"\n",
    "# Adil: \"It’s great for handling complex data and avoiding overfitting.\"\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Train the Random Forest model\n",
    "rf_model = OneVsRestClassifier(RandomForestClassifier())\n",
    "rf_model.fit(X_train_dense, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_rf = rf_model.predict(X_test_dense)\n",
    "\n",
    "# Evaluate results\n",
    "print(\"\\nRandomForestClassifier Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adil Explains:\n",
    "> “Random Forest creates many decision trees and averages their predictions to be more accurate and stable.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adil: \"Harry, we’ve trained our models — now let’s save them!\"\n",
    "# Adil: \"That way, we won’t need to retrain every time we want to use them.\"\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save the TF-IDF vectorizer\n",
    "pickle.dump(tfidf, open('tfidf.pkl', 'wb'))\n",
    "\n",
    "# Save the trained classifier model (using the best one – here, SVC)\n",
    "pickle.dump(svc_model, open('clf.pkl', 'wb'))\n",
    "\n",
    "# Save the label encoder for decoding predicted categories later\n",
    "pickle.dump(le, open(\"encoder.pkl\", 'wb'))\n",
    "\n",
    "print(\"All components saved successfully!\")# Adil: \"Harry, we’ve trained our models — now let’s save them!\"\n",
    "# Adil: \"That way, we won’t need to retrain every time we want to use them.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adil: \"Now Harry, let’s write a function that takes any resume text and tells us the job category!\"\n",
    "# Adil: \"We’ll clean it, vectorize it, predict it, and decode the result.\"\n",
    "\n",
    "def predict_resume_category(input_resume):\n",
    "    # Step 1: Clean the resume text\n",
    "    cleaned_text = cleanResume(input_resume)\n",
    "    \n",
    "    # Step 2: Vectorize using trained TF-IDF model\n",
    "    vectorized_text = tfidf.transform([cleaned_text])\n",
    "    \n",
    "    # Step 3: Convert to dense array (if necessary for classifier)\n",
    "    vectorized_text = vectorized_text.toarray()\n",
    "    \n",
    "    # Step 4: Predict using trained SVC model\n",
    "    predicted_label = svc_model.predict(vectorized_text)\n",
    "    \n",
    "    # Step 5: Convert label back to category name\n",
    "    predicted_category = le.inverse_transform(predicted_label)\n",
    "\n",
    "    return predicted_category[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_resume = \"\"\"Experienced software engineer skilled in Python, machine learning, and data analysis. Worked on various AI projects...\"\"\"\n",
    "print(\"Predicted Category:\", predict_resume_category(sample_resume))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myresume = \"\"\"\n",
    "John Doe is an experienced Network Security Engineer with over 7 years of expertise in designing, implementing, and managing network security infrastructures. Specializing in safeguarding critical network systems, John has worked with various organizations to protect against cyber threats, data breaches, and unauthorized access. He is proficient in deploying firewalls, intrusion detection systems (IDS), VPNs, and network monitoring tools to ensure the integrity and security of networks.\n",
    "\n",
    "John holds a degree in Computer Science and certifications in several cybersecurity domains, including Certified Information Systems Security Professional (CISSP), Certified Ethical Hacker (CEH), and Cisco Certified Network Associate (CCNA). He has extensive experience in troubleshooting and resolving network vulnerabilities, and has played a key role in conducting security audits and risk assessments.\n",
    "\n",
    "Key Skills:\n",
    "- Network Security Architecture\n",
    "- Firewall Management and Configuration\n",
    "- Intrusion Detection and Prevention Systems (IDS/IPS)\n",
    "- Virtual Private Networks (VPNs)\n",
    "- Security Audits and Risk Assessments\n",
    "- Cybersecurity Incident Response\n",
    "- Network Monitoring and Traffic Analysis\n",
    "- Vulnerability Assessment and Penetration Testing\n",
    "- Data Encryption and Secure Communications\n",
    "\n",
    "Certifications:\n",
    "- CISSP (Certified Information Systems Security Professional)\n",
    "- CEH (Certified Ethical Hacker)\n",
    "- CCNA (Cisco Certified Network Associate)\n",
    "- CompTIA Security+\n",
    "\n",
    "Education:\n",
    "BSc in Computer Science, XYZ University, 2012-2016\n",
    "\n",
    "Professional Experience:\n",
    "- Network Security Engineer at ABC Corp (2016-Present)\n",
    "- IT Security Specialist at DEF Solutions (2014-2016)\n",
    "\n",
    "Languages:\n",
    "- English (Fluent)\n",
    "- French (Intermediate)\n",
    "\"\"\"\n",
    "\n",
    "# Now, test the model with the Network Security Engineer-focused resume\n",
    "predict_resume_category(myresume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myresume = \"\"\"I am a data scientist specializing in machine\n",
    "learning, deep learning, and computer vision. With\n",
    "a strong background in mathematics, statistics,\n",
    "and programming, I am passionate about\n",
    "uncovering hidden patterns and insights in data.\n",
    "I have extensive experience in developing\n",
    "predictive models, implementing deep learning\n",
    "algorithms, and designing computer vision\n",
    "systems. My technical skills include proficiency in\n",
    "Python, Sklearn, TensorFlow, and PyTorch.\n",
    "What sets me apart is my ability to effectively\n",
    "communicate complex concepts to diverse\n",
    "audiences. I excel in translating technical insights\n",
    "into actionable recommendations that drive\n",
    "informed decision-making.\n",
    "If you're looking for a dedicated and versatile data\n",
    "scientist to collaborate on impactful projects, I am\n",
    "eager to contribute my expertise. Let's harness the\n",
    "power of data together to unlock new possibilities\n",
    "and shape a better future.\n",
    "Contact & Sources\n",
    "Email: 611noorsaeed@gmail.com\n",
    "Phone: 03442826192\n",
    "Github: https://github.com/611noorsaeed\n",
    "Linkdin: https://www.linkedin.com/in/noor-saeed654a23263/\n",
    "Blogs: https://medium.com/@611noorsaeed\n",
    "Youtube: Artificial Intelligence\n",
    "ABOUT ME\n",
    "WORK EXPERIENCE\n",
    "SKILLES\n",
    "NOOR SAEED\n",
    "LANGUAGES\n",
    "English\n",
    "Urdu\n",
    "Hindi\n",
    "I am a versatile data scientist with expertise in a wide\n",
    "range of projects, including machine learning,\n",
    "recommendation systems, deep learning, and computer\n",
    "vision. Throughout my career, I have successfully\n",
    "developed and deployed various machine learning models\n",
    "to solve complex problems and drive data-driven\n",
    "decision-making\n",
    "Machine Learnine\n",
    "Deep Learning\n",
    "Computer Vision\n",
    "Recommendation Systems\n",
    "Data Visualization\n",
    "Programming Languages (Python, SQL)\n",
    "Data Preprocessing and Feature Engineering\n",
    "Model Evaluation and Deployment\n",
    "Statistical Analysis\n",
    "Communication and Collaboration\n",
    "\"\"\"\n",
    "\n",
    "predict_resume_category(myresume)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1177531,
     "sourceId": 1971405,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
